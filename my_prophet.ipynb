{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Paramètres en durs","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fe5ccce-4e08-4afc-b773-e4245a7f8b4d"}}},{"cell_type":"markdown","source":"### Dictionnaires Métriques","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e9ebaa1-f6ec-4991-8283-399f2c48d1fe"}}},{"cell_type":"code","source":"dico_tempo = {'w':[7, 'W-MON'], 'm': [30, 'M'], 'y': [365, 'Y']}\ndico_metrics = {'explained_var':metrics.explained_variance_score,\n               'max_error': metrics.max_error,\n                'mae' : mean_absolute_error,\n                'geometric_absolute_error' : geometric_mean_absolute_error,\n                'geometric_mean_squared_error' : geometric_mean_squared_error,\n                'mape' : mean_absolute_percentage_error,\n               'mse' : mean_absolute_percentage_error}\n","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ab05b06-838f-46aa-8b52-a0e8f7286e18"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dictionnaires de paramètres fixes des modèles","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44ea0c88-303b-470d-95bd-219dd5f19c46"}}},{"cell_type":"code","source":"def get_liste_jferies(annee_debut=2017, annee_fin=2030):\n  liste = []\n  for annee in range(annee_debut, annee_fin+1):\n    dico = JoursFeries.for_year(annee)\n    for date in dico.values():\n      liste.append(date)\n  return liste","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a7bfbde-531c-4f6c-a7ab-3c8014af33ce"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\n\n# Dataframe des jours fériés pour Prophet\njferies = pd.DataFrame({\n'holiday': 'jferies',\n'ds': pd.to_datetime(get_liste_jferies(2017, 2040)),\n'lower_window': 0,\n'upper_window': 1,\n })\n\ndico_params = {'Prophet' : {'growth' : ['linear'], 'yearly_seasonality' : [True], 'weekly_seasonality' : [True], 'daily_seasonality' : [False], 'seasonality_mode' : ['additive', 'multiplicative'], 'seasonality_prior_scale' : [10, 20, 30, 50], 'changepoint_prior_scale' : [0.02, 0.03, 0.04, 0.05]}, \n               \n               'Prophet_opti' : {'seasonality_mode' : ['additive', 'multiplicative'], 'changepoint_prior_scale' : [0.05, 0.01, 0.005, 0.001], 'holidays_prior_scale' : [10, 1, 0.1], 'changepoint_range' : [0.7, 0.8], 'n_changepoints' : [10], 'yearly_seasonality' : [True], 'weekly_seasonality' : [True], 'holidays' : [jferies], 'growth' : ['linear']},\n               \n               'SARIMAX' : {'start_p': 0, 'start_q': 0 , 'max_p': 3, 'max_q': 3, 'start_P': 0, 'start_Q': 0, 'max_P': 2, 'max_D': 2, 'max_Q': 2, 'sp': 52}, \n              'MSTL' : {'level' : ['dtrend'], 'stochastic_trend': [False], 'stochastic_level': [False], 'seasonal' : [7],'irregular': [False], 'freq_seasonal' : [[{'period': 31, 'harmonics': 4}, {'period': 365, 'harmonics': 8}]]},\n               'Expo' : {'trend':['additive', None], 'seasonal' :['additive'], 'sp' :[364]},  \n              'Theta' :{'sp' : [364]}, \n              'RL' : {'fourier' : [4, 6, 8, 10, 12, 14, 16, 20, 24], 'start_date' : [datetime(2017,6,1), datetime(2018,1,1), datetime(2018,7,1), datetime(2019,1,1), datetime(2019,7,1)]}}","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c0015c3-f798-40fc-ae00-abf253473bce"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fonctions création base ou features","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fdc0d1f-288e-40ed-80f7-cac3355532d8"}}},{"cell_type":"code","source":"def abs_tempo(df_temp, tempo='w'):\n  \"\"\" Prend en entrée le df df_temp etv^$(kk) retourne un df avec la DATE en index et y le taux d'absentéisme agrégé par temporalité \"\"\"\n  ts = df_temp.copy()\n  ts = ts.sort_values('DATE')\n  if tempo == 'd': \n    ts.index = ts['DATE']\n    ts = ts.drop(['DATE', 'NB_ACTIFS', 'NB_INACTIFS'], axis=1)\n    ts = ts.asfreq('d')\n    ts.columns = ['y']\n  else : \n    ts = ts.drop(columns=['TX_ABS'])\n    ts['DATE'] = pd.to_datetime(ts['DATE']) - pd.to_timedelta(dico_tempo[tempo][0], unit='d')\n\n    ts = ts.groupby([pd.Grouper(key='DATE', freq=dico_tempo[tempo][1])]).sum().reset_index().sort_values('DATE')\n    ts['y'] = ts['NB_INACTIFS']/ts['NB_ACTIFS']\n    ts.index = ts.DATE \n    ts = ts.drop(columns=['DATE'])\n    ts.index.freq = dico_tempo[tempo][1]\n    ts = ts.drop(columns=['NB_ACTIFS', 'NB_INACTIFS'])\n  return(ts)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03313ed5-74e2-4e05-a047-f8db011afe89"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jours_feries_france import JoursFeries\n\ndef is_ferie(row):\n  return int(JoursFeries.is_bank_holiday(row.name, zone=\"Métropole\"))\n\ndef is_ferie_demain(row):\n  return int(JoursFeries.is_bank_holiday(row.name + timedelta(days=1), zone=\"Métropole\"))\n\ndef is_ferie_apresdemain(row):\n  return int(JoursFeries.is_bank_holiday(row.name + timedelta(days=2), zone=\"Métropole\"))\n\ndef is_ferie_hier(row):\n  return int(JoursFeries.is_bank_holiday(row.name - timedelta(days=1), zone=\"Métropole\"))\n\ndef is_ferie_avanthier(row):\n  return int(JoursFeries.is_bank_holiday(row.name - timedelta(days=2), zone=\"Métropole\"))\n\ndef is_ferie_apresapresdemain(row):\n   return int(JoursFeries.is_bank_holiday(row.name + timedelta(days=3), zone=\"Métropole\"))\n\ndef is_ferie_avantavanthier(row):\n   return int(JoursFeries.is_bank_holiday(row.name - timedelta(days=3), zone=\"Métropole\"))\n  \ndef period_to_datetime(period):\n  return datetime(period.year, period.month, period.day)\n\ndef confinement(row):\n  #intercept for linear model\n  day = period_to_datetime(row.name)\n  confinement1 = day >= datetime(2020,3,17) and day < datetime(2020,5,11)\n  confinement2 = day >= datetime(2020,10,30) and day < datetime(2020,12,15)\n  confinement3 = day >= datetime(2021,4,3) and day < datetime(2021,5,3)\n  return int(confinement1 or confinement2 or confinement3)\n\ndef confinement1(row):\n  start = datetime(2020,3,17)\n  end = datetime(2020,5,11)\n  day = period_to_datetime(row.name)\n  if day >= start and day < end:\n    a = (day - start).days\n    b = (end - start).days\n    return a/b\n  else:\n    return 0\n\ndef confinement2(row):\n  start = datetime(2020,10,30)\n  end = datetime(2020,12,15)\n  day = period_to_datetime(row.name)\n  if day >= start and day < end:\n    a = (day - start).days\n    b = (end - start).days\n    return a/b\n  else:\n    return 0\n\ndef confinement3(row):\n  start = datetime(2021,4,3)\n  end = datetime(2021,5,3)\n  day = period_to_datetime(row.name)\n  if day >= start and day < end:\n    a = (day - start).days\n    b = (end - start).days\n    return a/b\n  else:\n    return 0\n\ndef confinement1_intercept(row):\n  start = datetime(2020,3,17)\n  end = datetime(2020,5,11)\n  day = period_to_datetime(row.name)\n  return  int(day >= start and day < end)\n\ndef confinement2_intercept(row):\n  start = datetime(2020,10,30)\n  end = datetime(2020,12,15)\n  day = period_to_datetime(row.name)\n  return  int(day >= start and day < end)\n\ndef confinement3_intercept(row):\n  start = datetime(2021,4,3)\n  end = datetime(2021,5,3)\n  day = period_to_datetime(row.name)\n  return  int(day >= start and day < end)\n  \ndef confinement123(row):\n  start = datetime(2020,3,17)\n  end = datetime(2020,5,11)\n  if row.name >= start and row.name < end:\n    a = (row.name - start).days\n    b = (end - start).days\n    return a/b\n  else:\n      start = datetime(2020,10,30)\n      end = datetime(2020,12,15)\n      if row.name >= start and row.name < end:\n        a = (row.name - start).days\n        b = (end - start).days\n        return a/b\n      else:\n          start = datetime(2021,4,3)\n          end = datetime(2021,5,3)\n          if row.name >= start and row.name < end:\n            a = (row.name - start).days\n            b = (end - start).days\n            return a/b\n          else:\n            return 0\n\ndef covid_changepoint(row):\n  return int(row.name >= datetime(2020,3,17))\n\nfrom vacances_scolaires_france import SchoolHolidayDates\nd = SchoolHolidayDates()\ndef is_holidays(row):\n  return int(d.is_holiday_for_zone(datetime.date(row.name), 'A') and d.is_holiday_for_zone(datetime.date(row.name), 'B') and d.is_holiday_for_zone(datetime.date(row.name), 'C'))","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef4547a5-af69-4b0c-9cb7-40f46ab5444d"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Find Start Date","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"352304f1-598c-4256-b431-4593db23598a"}}},{"cell_type":"code","source":"def create_split_dates(ts_, real_split_date):\n  ts = ts_.copy()\n  ts = ts.sort_values('DATE')\n  ts.index = ts['DATE']\n  ts = ts.drop(['DATE', 'NB_SALARIE'], axis=1)\n  return pd.date_range(ts.index.min(), real_split_date, freq='m')\n\ndef find_start_date_trend(ts_, real_split_date):\n  \"\"\"\n  Trouve la start_date à partir de laquelle les données sont potables en se basant sur la variation de tendace avec start_date <= real_split_date\n  \"\"\"\n  \n  #Preprocessing\n  ts = ts_.copy()\n  ts = ts.sort_values('DATE')\n  ts.index = ts['DATE']\n  ts = ts.drop(['DATE', 'NB_SALARIE'], axis=1)\n  ts = ts.asfreq(\"D\")\n  ts.columns = ['y']\n  confinement1 = pd.date_range('2020-03-10', '2020-05-10')\n  confinement2 = pd.date_range('2020-10-30', '2020-12-14')\n  confinement3 = pd.date_range('2021-04-03', '2021-05-02')\n  ts.loc[confinement1] = np.nan\n  ts.loc[confinement2] = np.nan\n  ts.loc[confinement3] = np.nan\n  ts = ts.fillna(method='backfill')\n  \n  #create splitdates\n  split_dates = create_split_dates(ts_, real_split_date)\n  best_split_date = split_dates[0]\n  max_rupture = 0\n  for split_date in split_dates:\n    #Création des dates\n    train_start = ts.index.min()\n    train_end = split_date\n    val_start = split_date + timedelta(days=1)\n    val_end = ts.index.max()\n\n    #Preprocessing\n#     split_date_lend = pd.to_datetime(split_date) + timedelta(days=1)\n#     fh_val = ForecastingHorizon(pd.period_range(start= split_date_lend, end=ts.index.max(), freq=\"d\"), is_relative=False)\n#     fh_train = ForecastingHorizon(pd.period_range(start=ts.index.min(), end=split_date, freq=\"d\"), is_relative=False)\n    y = ts[['y']]\n    y.index = pd.period_range(start=ts.index.min(),end=ts.index.max(), freq=\"d\")\n    y_train, y_val = y[train_start:train_end], y[val_start:val_end]\n\n    X_idx = pd.period_range(start=train_start,end=val_end , freq=\"d\")\n    train_idx = pd.period_range(start=train_start,end=train_end , freq=\"d\")\n    val_idx = pd.period_range(start=val_start,end=val_end , freq=\"d\")\n\n    dp = DeterministicProcess(\n        index=X_idx,\n        constant=False,               # dummy feature for bias (y-intercept)\n        order=1,                     # trend (order 1 means linear)\n        seasonal=False,               # weekly seasonality (indicators)\n    )\n    X = dp.in_sample() \n    X_train = X[train_start: train_end]\n    X_val = X[val_start: val_end]\n\n    #Evaluation sur la validation\n    model_train = LinearRegression(fit_intercept=True)\n    model_train.fit(X_train, y_train['y'])\n    model_val = LinearRegression(fit_intercept=True)\n    model_val.fit(X_val, y_val['y'])\n    rupture = np.abs(model_train.coef_[0] - model_val.coef_[0])\n    \n    if rupture > max_rupture:\n      max_rupture = rupture\n      best_split_date = split_date\n\n  \n  return best_split_date","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b675d8a-6126-424e-9c16-82bb5c0e4361"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_start_date_variance(y, tune=0.5, cut=31):\n  \"\"\"\n  Trouve la start_date à partir de laquelle les données sont potables en se basant sur la variation de variance avec start_date <= real_split_date\n  \n  tune vaut entre 0 et 1. Plus tune est élevé, plus on choisira une date tard dans le temps.\n  \"\"\"\n  \n  #Preprocessing\n  ts = y.copy()\n  ts['rolling_small'] = ts['y'].rolling(window=3, center=True).mean()\n  ts['rolling'] = ts['y'].rolling(window=182, center=True, min_periods=91).mean()\n  ts['variance'] = np.abs(ts['rolling_small'] - ts['rolling'])\n  ts = ts.fillna(method='ffill')\n  ts = ts.fillna(method='backfill')\n  \n  #Eventuellement mettre un if pour check return date <= split_date\n  start_date = ts.index[(ts['variance'] >= ts['variance'].max() * tune).argmax()]\n  total_length = len(pd.period_range(start_date, ts.index.max()))\n  if total_length < 365:\n    print(f\"WARNING : La longueur du dataset d'entrainement est inférieur à un an : {total_length} jours\")\n    print(\"start_date = \", start_date)\n  \n#   print(\"start_date = \", start_date)\n  return start_date","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96378809-331f-42fd-9726-82e2ba14bd96"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(ts_, omicron = True, tune=0.5, cut=31):\n  \"\"\"\n  Prétraite l'effet covid du 1er confinement et du variant omicron (debut 2022 jusqu'à mi fevrier 2022) et renvoie la serie temporelle commençant à partir de la date du début de déclaration. \n  Avec la méthode 'replace', cette fonction remplace le 1er pic du covid par la prédiction d'une régression linéaire avec 6 paires de series de fourier en s'entrainant sur la période  '2020-01-10' - '2020-03-10' union '2020-05-10' - '2020-07-10'\n  Avec la méthode 'remove', cette fonction remplace le 1er pic du covid par des NaN pour les modèles qui peuvent traiter des valeurs manquantes.\n  La sortie est un dataframe d'index de type pd.period_range(start=start,end=end , freq=\"d\").\n  \n  Paramètres :\n  \n  ts_ : dataframe\n    Série temporelle format pickle 2\n  \n  method : {'replace', 'remove', 'nothing'}\n    Désigne la méthode de traitement du covid\n    \n  omicron : bool\n    Si True, traiter le pic d'absenteisme du à Omicron en 2022\n    \n  cut : int\n    Coupe les 'cut' derniers pas de temps à cause de la date de fin de déclaration\n    \n  \"\"\"\n  \n  #Preprocessing\n  ts = ts_.copy()\n  ts = ts.sort_values('DATE')\n  ts.index = ts['DATE']\n  ts = ts.drop(['DATE', 'NB_ACTIFS', 'NB_INACTIFS'], axis=1)\n  ts = ts.asfreq(\"D\")\n  ts.columns = ['y']\n  \n  #Création des dates\n  start = ts.index.min()\n  \n  confinement1_deb = pd.to_datetime('2020-03-10')\n  confinement1_fin = pd.to_datetime('2020-05-10')\n  train1_start = pd.to_datetime('2020-01-10')\n  train1_end = confinement1_deb - timedelta(days=1)\n  train1_after_start = confinement1_fin + timedelta(days=1)\n  train1_after_end = pd.to_datetime('2020-07-10')\n  val1_start = confinement1_deb\n  val1_end = confinement1_fin\n  \n  confinement2_deb = pd.to_datetime('2021-12-24')\n  confinement2_fin = pd.to_datetime('2022-02-14')\n  train2_start = pd.to_datetime('2021-10-01')\n  train2_end = confinement2_deb - timedelta(days=1)\n  train2_after_start = confinement2_fin + timedelta(days=1)\n  train2_after_end = pd.to_datetime('2022-03-14')\n  val2_start = confinement2_deb\n  val2_end = confinement2_fin\n  \n  end = ts.index.max()\n\n  #Preprocessing\n  y = ts['y']\n  y.index = pd.period_range(start=ts.index.min(),end=ts.index.max(), freq=\"d\")\n  real = y.copy()\n  X_idx = pd.period_range(start=start, end=end , freq=\"d\")\n  \n  train1_idx = pd.period_range(start=train1_start, end=train1_end, freq=\"d\").union(pd.period_range(start=train1_after_start, end=train1_after_end, freq=\"d\"))\n  val1_idx = pd.period_range(start=val1_start, end=val1_end, freq=\"d\")\n  \n  train2_idx = pd.period_range(start=train2_start, end=train2_end, freq=\"d\").union(pd.period_range(start=train2_after_start, end=train2_after_end, freq=\"d\"))\n  val2_idx = pd.period_range(start=val2_start, end=val2_end, freq=\"d\")\n  \n  #Création du modèle final \n  fourier = CalendarFourier(freq=\"M\", order=6)  #number of sin/cos pairs for \"A\"nnual seasonality\n  dp = DeterministicProcess(\n      index=X_idx,\n      constant=False,               # dummy feature for bias (y-intercept)\n      order=1,                     # trend (order 1 means linear)\n      seasonal=True,               # weekly seasonality (indicators)\n      additional_terms=[fourier],  # annual seasonality (fourier)\n      drop=True,                   # drop terms to avoid collinearity\n  )\n  X = dp.in_sample() \n  X['isferie'] = X.apply(is_ferie, axis=1)\n  X['isferie_demain'] = X.apply(is_ferie_demain, axis=1)\n  X['isferie_hier'] = X.apply(is_ferie_hier, axis=1)\n  X['isferie_apresdemain'] = X.apply(is_ferie_apresdemain, axis=1)\n  X['isferie_avanthier'] = X.apply(is_ferie_avanthier, axis=1)\n  X['isferie_apresapresdemain'] = X.apply(is_ferie_apresapresdemain, axis=1)\n  X['isferie_avantavanthier'] = X.apply(is_ferie_avantavanthier, axis=1)\n\n  #1er confinement\n  try:\n    X_train1 = X.loc[train1_idx]\n    X_val1 = X[val1_start: val1_end]\n    model = LinearRegression(fit_intercept=True)\n    model.fit(X_train1, y.loc[train1_idx])\n    y_pred1 = model.predict(X_val1)\n    y_pred1 = pd.Series(y_pred1, name='y_pred1', index=X_val1.index)\n    y.loc[val1_idx] = y_pred1\n  except Exception:\n    pass\n  #2eme confinement\n  if omicron:\n    try:\n      X_train2 = X.loc[train2_idx]\n      X_val2 = X[val2_start: val2_end]\n      model = LinearRegression(fit_intercept=True)\n      model.fit(X_train2, y.loc[train2_idx])\n      y_pred2 = model.predict(X_val2)\n      y_pred2 = pd.Series(y_pred2, name='y_pred2', index=X_val2.index)\n      y.loc[val2_idx] = y_pred2\n    except Exception:\n      pass\n\n  #Décommenter la ligne en bas pour comparer avec la série temporelle initiale\n#     y['real'] = real\n\n  #Debut des declarations\n  y = y.to_frame('y')\n  y = y.iloc[:-cut]\n  start_date = find_start_date_variance(y, tune=tune, cut=cut)\n  y = y[start_date:]\n\n  #partie remove\n  y_nan = y.copy()\n#   val1_idx = pd.period_range(start=val1_start, end=val1_end, freq=\"d\")\n#   y_nan.loc[val1_idx] = np.nan\n#   if omicron:\n#     val2_idx = pd.period_range(start=val2_start, end=val2_end, freq=\"d\")\n#     y_nan.loc[val2_idx] = np.nan\n\n  #partie nothing\n#   print(\"real_start_date = \", ts.index.min())\n  y_real = ts['y']\n  y_real = y_real.to_frame('y')\n  y_real = y_real[start_date:]\n  y_real = y_real.iloc[:-cut]\n\n  return y, y_nan, y_real","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"564df1aa-5019-4c8d-87bd-0c6e4e7c7fe2"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SARIMAX","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9f021b1-4537-4f55-aee6-4f5643e54126"}}},{"cell_type":"code","source":"import re","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51789b85-f5a7-495d-b382-47a4224c19fa"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_features_sarimax(idx):\n  X = pd.DataFrame({'DATE' : idx})\n  X.index = X.DATE\n  X['isferie'] = X.apply(is_ferie, axis=1)\n  X['isferie_demain'] = X.apply(is_ferie_demain, axis=1)\n  X['isferie_hier'] = X.apply(is_ferie_hier, axis=1)\n  X['isferie_apresdemain'] = X.apply(is_ferie_apresdemain, axis=1)\n  X['isferie_avanthier'] = X.apply(is_ferie_avanthier, axis=1)\n  X['isferie_apresapresdemain'] = X.apply(is_ferie_apresapresdemain, axis=1)\n  X['isferie_avantavanthier'] = X.apply(is_ferie_avantavanthier, axis=1)\n  X = X.drop(columns=['DATE'])\n  X.index = X.index.to_timestamp()\n  X = X.rolling(window=7, center=True).mean()\n  X = X.apply(lambda x: x*7)\n  fourier = CalendarFourier(freq=\"A\", order=10)  #number of sin/cos pairs for \"A\"nnual seasonality\n  dp = DeterministicProcess(\n      index=idx,\n      constant=False,               # dummy feature for bias (y-intercept)\n      order=1,                     # trend (order 1 means linear)\n      seasonal=True,               # weekly seasonality (indicators)\n      additional_terms=[fourier],  # annual seasonality (fourier)\n      drop=False,                   # drop terms to avoid collinearity\n  )\n  Y = dp.in_sample() \n  Y = Y.iloc[:,8:]\n  Y.index = Y.index.to_timestamp()\n  Z = X.merge(Y,how='inner', left_index=True, right_index=True)\n  Z = Z.fillna(0)\n  return Z","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6e73743-4143-41b5-a379-3ce88638aefc"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_sarimax(timeseries,val_size, steps=365): \n  \n  param = dico_params['SARIMAX']\n    \n  #Retraitement + calcul de la base agrégée par semaine \n  pred_dico = {'val_size' : val_size, 'steps' : int(steps/7)}\n  ts,_,_ = preprocess_data(timeseries)\n  ts.index = ts.index.to_timestamp()\n  ts_merged = ts.merge(timeseries, how='inner', left_index=True, right_on = 'DATE')\n  ts_merged['NB_INACTIFS'] = ts_merged['y']*ts_merged['NB_ACTIFS']\n  ts_merged = ts_merged.drop(columns=['y', 'TX_ABS']).reset_index(drop=True)\n  ts_merged['DATE'] = pd.to_datetime(ts_merged['DATE']) - pd.to_timedelta(7, unit='d')\n  ts = ts_merged.groupby([pd.Grouper(key='DATE', freq='W')])['NB_ACTIFS', 'NB_INACTIFS'].sum()\n  ts['y'] = ts['NB_INACTIFS']/ts['NB_ACTIFS']\n  ts = ts.drop(columns=['NB_ACTIFS', 'NB_INACTIFS'])\n  y = ts[['y']]\n  y_train, y_val = temporal_train_test_split(y, test_size = pred_dico['val_size'])\n  \n  #horizons de prédiction\n  fh_train = ForecastingHorizon(y_train.index, is_relative=False)\n  fh_val = ForecastingHorizon(y_val.index, is_relative=False)\n  fh_test = ForecastingHorizon(pd.date_range(start=y_val.index.max() + timedelta(7) ,periods=pred_dico['steps'] , freq=\"W-SUN\"), is_relative=False)\n  df_train = y_train.copy()\n  df_val = y_val.copy()\n  \n  #AutoARIMA pour recherche des best_params \n  forecaster = AutoARIMA(start_p=param['start_p'], start_q = param['start_q'],max_p = param['max_p'], max_q=param['max_q'], start_P = param['start_P'],start_Q = param['start_Q'],\n                                    max_P=param['max_P'], max_D = param['max_D'],max_Q = param['max_Q'],sp = param['sp'], n_jobs=-1)\n  forecaster.fit(y_train)\n  \n  summary_string = str(forecaster.summary())\n  try: \n    param = re.findall('SARIMAX\\(([0-9]+), ([0-9]+), ([0-9]+)',summary_string)\n    ar_order = (int(param[0][0]) , int(param[0][1]) , int(param[0][2]))\n  except : \n    ar_order = (0,0,0)\n\n  try : \n    param_s = re.findall('x\\(([0-9]+), ([0-9]+), ([0-9]+), ([0-9]+)',summary_string)\n    season_order = (int(param_s[0][0]) , int(param_s[0][1]) , int(param_s[0][2]), int(param_s[0][3]))\n  except : \n    try:\n      param_s = re.findall('x\\(([0-9]+), ([0-9]+), \\[([0-9]+)\\], ([0-9]+)',summary_string)\n      season_order = (int(param_s[0][0]) , int(param_s[0][1]) , int(param_s[0][2]), int(param_s[0][3]))\n    except : \n      season_order =(0,0,0,0)\n  dict_bp = {'order' : ar_order, 'seasonal_order' : season_order}\n  \n  train_start = y_train.index.min()\n  train_end = y_train.index.max()\n  val_start = y_val.index.min()\n  val_end = y_val.index.max()\n  test_start = val_end + timedelta(days=7)\n  test_end = val_end + timedelta(days=7) + timedelta(days=steps)\n  X_idx = pd.period_range(start=train_start,end=test_end , freq=\"d\")\n  \n  features = create_features_sarimax(X_idx)\n  y_features = y.merge(features, how='inner', left_index=True, right_index = True).drop(columns=['y'])\n  train_features = y_train.merge(features, how='inner', left_index=True, right_index = True).drop(columns=['y'])\n  val_features = y_val.merge(features, how='inner', left_index=True, right_index = True).drop(columns=['y'])\n  test_features = pd.DataFrame(index=pd.date_range(start=y_val.index.max() + timedelta(7) ,periods=pred_dico['steps'], freq=\"W-SUN\")).merge(features, how='inner', left_index=True, right_index = True)\n  \n  forecaster = SARIMAX(order=dict_bp['order'], seasonal_order=dict_bp['seasonal_order'])\n  forecaster.fit(y_train, train_features)\n  train = forecaster.predict(fh_train, X =train_features).rename(columns = {'y' : 'pred'})\n  val = forecaster.predict(fh=fh_val, X=val_features).rename(columns = {'y' : 'pred'})\n  forecaster = SARIMAX(order=dict_bp['order'], seasonal_order=dict_bp['seasonal_order'])\n  forecaster.fit(y, y_features)\n  test = forecaster.predict(fh_test, X =test_features).rename(columns = {'y' : 'pred'})\n\n  best_params = dict_bp\n \n  train.index = train.index.to_period(freq='d')\n  val.index = val.index.to_period(freq='d')\n  test.index = test.index.to_period(freq='d')\n   \n  \n  return(best_params, train, val, test)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ec08ad7-4bb0-4e32-97e4-2f174ce9859e"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modèle RL","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3e42838-ed39-4dc4-a9a1-68829f239e3a"}}},{"cell_type":"code","source":"def create_features(idx, covid_=False, drop=True, mois_=False, order=10):\n  fourier = CalendarFourier(freq=\"A\", order=order)  #number of sin/cos pairs for \"A\"nnual seasonality\n  dp = DeterministicProcess(\n      index=idx,\n      constant=False,               # dummy feature for bias (y-intercept)\n      order=1,                     # trend (order 1 means linear)\n      seasonal=True,               # weekly seasonality (indicators)\n      additional_terms=[fourier],  # annual seasonality (fourier)\n      drop=drop,                   # drop terms to avoid collinearity\n  )\n  X = dp.in_sample() \n  X['isferie'] = X.apply(is_ferie, axis=1)\n  X['isferie_demain'] = X.apply(is_ferie_demain, axis=1)\n  X['isferie_hier'] = X.apply(is_ferie_hier, axis=1)\n  X['isferie_apresdemain'] = X.apply(is_ferie_apresdemain, axis=1)\n  X['isferie_avanthier'] = X.apply(is_ferie_avanthier, axis=1)\n  X['isferie_apresapresdemain'] = X.apply(is_ferie_apresapresdemain, axis=1)\n  X['isferie_avantavanthier'] = X.apply(is_ferie_avantavanthier, axis=1)\n  if covid_:\n    X['covid'] = X.apply(covid, axis=1)\n  if mois_:\n    X['mois'] = X.apply(mois, axis=1)\n    X['day'] = X.apply(day, axis=1)\n#     X['confinement'] = X.apply(confinement, axis=1)\n#   X['confinement1'] = X.apply(confinement1, axis=1)\n#   X['confinement2'] = X.apply(confinement2, axis=1)\n#   X['confinement3'] = X.apply(confinement3, axis=1)\n#   X['confinement1_intercept'] = X.apply(confinement1_intercept, axis=1)\n#   X['confinement2_intercept'] = X.apply(confinement2_intercept, axis=1)\n#   X['confinement3_intercept'] = X.apply(confinement3_intercept, axis=1)\n  #---Features supplémentaires non utilisés car peuvent empirer la prédiction---\n  # X['confinement123'] = X.apply(confinement123, axis=1)\n  # X['is_confinement1'] = X.apply(is_confinement1, axis=1)\n  # X['is_confinement2'] = X.apply(is_confinement2, axis=1)\n  # X['is_confinement3'] = X.apply(is_confinement3, axis=1)\n  # X['covid_changepoint'] = X.apply(covid_changepoint, axis=1)\n  # X['isholidays'] = X.apply(is_holidays, axis=1)  #pas vraiment utile, redondant avec les vacs\n  return X","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6b1eefa-943d-4a69-8186-78e47ed10d7f"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom sklearn.linear_model import LinearRegression, Lasso\n\ndef PiecewiseLinearRegression(y, steps, window=365, n_changepoints=25, alpha=0.000005, verbose=False):\n  \"\"\"\n  Prend en argument la série temporelle prétraité et renvoie la prédiction (modèle de regression linéaire avec tendance affine par morceaux très similaire à prophet) sur le train et sur le val sur une durée de \"steps\" pas de temps. \n  \n  Parametres :\n  \n  y : dataframe\n    série temporelle prétraitée avec preprocess data\n  steps : int\n    nombre de pas de temps dans le futur sur lesquels effectuer la prédiction\n  window : int\n    on modélise la tendance en modèlisant la moyenne glissée sur un fenetre de taille window par une fonction affine par morceaux\n  n_changepoints : int\n    comme prophet on réparti n_changepoints dates de manière uniforme sur le dataset comme potentiels changepoints\n  alpha : float\n    paramètre alpha pour le modèle Lasso pour sélectionner les dates utilisées\n  verbose : bool\n    afficher la prediction sur le train vs la valeur reelle\n  \"\"\"\n  y = y['y']\n  y_smooth = y.rolling(window=window, center=True, min_periods=100).mean() #param\n  y_smooth = y_smooth.dropna()\n  idx = pd.period_range(start=y.index.min(), periods=len(y) + steps, freq=\"d\")\n  test_idx = pd.period_range(start=y.index.max() + timedelta(days=1), periods=steps, freq=\"d\")\n\n  dp = DeterministicProcess(\n      index=idx,\n      constant=False,               # dummy feature for bias (y-intercept)\n      order=1,                     # trend (order 1 means linear)\n      seasonal=False,               # weekly seasonality (indicators)\n  )\n\n  X = dp.in_sample()  # create features for dates in tunnel.index\n\n  changepoints = y_smooth.index[len(y_smooth.index)//(2*n_changepoints) : -len(y_smooth.index)//(2*n_changepoints) :len(y_smooth.index)//(n_changepoints)]\n  for i in range(len(changepoints)):\n    X['changepoint_trend_' + str(i)] = 0\n    n = len(pd.period_range(start=changepoints[i], end=idx.max(), freq='d'))\n    X.loc[changepoints[i]:, 'changepoint_trend_' + str(i)] = np.arange(n)\n  X = X / len(y.index)\n  X['trend'] *= 1000\n\n  model = Lasso(alpha=alpha) #param\n  #model = LinearRegression()\n  model.fit(X.loc[y_smooth.index], y_smooth) #on entraine le model sur la partie smooth\n  train_trend = model.predict(X.loc[y.index])\n  test_trend = model.predict(X.loc[test_idx])\n  train_trend = pd.DataFrame(data=train_trend, index=y.index, columns=['pred'])\n  test_trend = pd.DataFrame(data=test_trend, index=test_idx, columns=['pred'])\n  \n  X_feats = create_features(idx)\n  model = LinearRegression()\n  model.fit(X_feats.loc[y.index], y - train_trend['pred'])\n  train_pred = train_trend + pd.DataFrame(data=model.predict(X_feats.loc[y.index]), index=y.index, columns=['pred'])\n  test_pred = test_trend + pd.DataFrame(data=model.predict(X_feats.loc[test_idx]), index=test_idx, columns=['pred'])\n  \n  if verbose:\n    train_pred['real'] = y\n    ax = df.plot()\n    df_test.plot(ax=ax)\n    plt.show()\n  \n  return train_pred, test_pred\n\n\ndef model_rl(y, y_train, y_val, dico_pred):\n  \"\"\"\n  Modèle de regression linéaire très similaire à prophet avec une modélisation affine par morceaux de la tendance\n  Il ne possède aucun hyperparamètre à tuner.\n  \"\"\"\n  \n  steps = dico_pred['steps']\n  train, val = PiecewiseLinearRegression(y_train, len(y_val))\n  _, test = PiecewiseLinearRegression(y, steps)\n  best_params = {}\n  \n  return(best_params, train, val, test)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf9908df-a161-455a-9593-49991cddc8fb"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modèle Prophet","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ca211bb-582f-478c-a064-133b5a3f0c0b"}}},{"cell_type":"code","source":"def model_prophet_opti(y, y_train, y_val, dico_pred): \n  \n  y.index = y.index.to_timestamp()\n  y_val.index = y_val.index.to_timestamp()\n  y_train.index = y_train.index.to_timestamp()\n\n  cv = ExpandingWindowSplitter(initial_window = 2*365, step_length = int((len(y)-(200))/5),fh = 200)\n  param_grid = dico_params['Prophet']\n  gscv = ForecastingGridSearchCV(Prophet(), cv=cv, param_grid=param_grid,verbose=True, scoring = MeanSquaredError(square_root=True), n_jobs = -1)\n  \n  try : \n    gscv_train = gscv.fit(y, fh = y_train.index)\n  except : \n    print(\"Il faut réduire horizon de prédiction\")\n    \n  \n  best_params = gscv_train.best_params_\n  \n  prophet_mdl = Prophet().set_params(**best_params).fit(y_train, fh=y_val.index)\n  y_val_ = prophet_mdl.predict(fh=y_val.index).rename(columns={'y' : 'pred'})\n  train = prophet_mdl.predict(fh=y_train.index).rename(columns={'y' : 'pred'})\n  \n  prophet_final =   Prophet().set_params(**best_params).fit(y)\n  data_range = pd.DatetimeIndex(data = pd.period_range(start=pd.to_datetime(y.index.max()) + timedelta(days=1), periods = dico_pred['steps'],freq='d').to_timestamp())\n  y_test = prophet_final.predict(pd.DatetimeIndex(data = data_range, freq=\"d\"))\n  y_test = y_test.rename(columns = {'y' : 'pred'})\n  \n  \"\"\"df_val_ = pd.DataFrame(prophet_mdl.predict_interval(fh =pd.DatetimeIndex(data = y_val.index, freq=\"d\"), coverage=0.9)['Coverage'][0.9]).rename(columns= {'lower' : 'y_min', 'upper': 'y_max'})\n  df_test_ = pd.DataFrame(prophet_final.predict_interval(fh =pd.DatetimeIndex(data = data_range, freq=\"d\"), coverage=0.9)['Coverage'][0.9]).rename(columns= {'lower' : 'y_min', 'upper': 'y_max'})\n  val = pd.concat([df_val_, y_val_], axis=1)\n  test = pd.concat([y_test, df_test_], axis=1)\n  \"\"\" \n  \n  val = y_val_\n  test = y_test\n  train.index = y_train.index.to_period()\n  val.index = y_val.index.to_period()\n  test.index =  pd.period_range(start=pd.to_datetime(y.index.max()) + timedelta(days=1), periods = dico_pred['steps'],freq='d')\n  \n  y.index = y.index.to_period()\n  y_val.index = y_val.index.to_period()\n  y_train.index = y_train.index.to_period()\n  \n  return(best_params, train, val, test)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"193e97eb-3c60-4bc8-ac96-c50e7d64ca0a"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Plot","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f65d6555-e517-4315-a189-629a4bb50abc"}}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd ","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b0ef02f-2c77-404e-a5f8-558114ce4281"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_ts_df(train, val, test, name, figsize=(20,20), wdw_1 = 35, wdw_2 = 1):\n  \n  col_train = [x for x in train.columns if ('_min_' not in x) and ('max_' not in x)] \n  col_val = [x for x in val.columns if ('_min_' not in x) and ('max_' not in x)] \n  col_test = [x for x in test.columns if ('_min_' not in x) and ('max_' not in x)] \n  \n  df_train = train[col_train]\n  df_train.index = df_train.index.to_timestamp()\n  \n  df_val = val[col_val]\n  df_val.index = df_val.index.to_timestamp()\n  \n  df_test = test[col_test]\n  df_test.index = df_test.index.to_timestamp()\n  \n  list_colors = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", 'g', 'r', 'y']\n  \n  fig, ax = plt.subplots(nrows=2, ncols=1, figsize=figsize)\n  ax[0].set_title('Taux absentéisme : observé vs prédit pour le client : ' + name,fontsize=30, pad=20)\n  ax[0].set_ylabel('Taux absentéisme')\n  \n  ax[0].set_prop_cycle('color',list_colors[:(df_train.drop(columns='y')).shape[1]])\n  ax[1].set_prop_cycle('color',list_colors[:(df_train.drop(columns='y')).shape[1]])\n      \n  ax[0].plot(df_train.y.rolling(window=wdw_1).mean(), label='y', linewidth=3, linestyle=':', color='darkblue')\n  ax[0].plot(df_train.drop(columns='y').rolling(window=wdw_1).mean(),label=list(df_train.drop(columns='y').columns))\n  ax[0].plot(df_test.rolling(window=wdw_1).mean())\n  ax[0].legend(loc='best')\n  ax[0].plot(df_val.y.rolling(window=wdw_1).mean(), label='y', linewidth=3, linestyle=':', color='darkblue')\n  ax[0].plot(df_val.drop(columns='y').rolling(window=wdw_1).mean(),\n              label=list(df_val.drop(columns='y').columns))\n  min_y_values = min([df_val.min().min(),df_train.min().min(),df_test.min().min()])\n  max_y_values = max([df_val.max().max(),df_train.max().max(),df_test.max().max()])\n  ax[0].vlines(df_val.index.min() + pd.Timedelta(wdw_1/2,'D'), min_y_values, max_y_values,color='darkblue')\n  ax[0].vlines(df_test.index.min() + pd.Timedelta(wdw_1/2,'D'), min_y_values, max_y_values,color='darkblue')\n  ax[0].text(df_train.index.mean(),max_y_values,'Train',size=20,color='darkblue')\n  ax[0].text(df_val.index.mean(),max_y_values,'Val',size=20,color='darkblue')\n  ax[0].text(df_test.index.mean(),max_y_values,'Futur',size=20,color='darkblue')\n\n\n  ax[1].plot(df_val.y.rolling(window=wdw_2).mean(), label='y', linewidth=3, linestyle=':', color='darkblue')\n  ax[1].plot(df_val.drop(columns='y').rolling(window=wdw_2).mean(),\n              label=list(df_val.drop(columns='y').columns))\n  ax[1].set_xlim([df_val.index.min(),df_val.index.min() + pd.Timedelta(182,'D')])\n  ax[1].legend(loc='best')\n  ax[1].set_title('Zoom sur le Val',fontsize=30, pad=20)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6521d18-f333-456b-ad2f-eb7f6763bcda"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calcul métriques","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc1d4ed6-5fd7-432a-a9e2-a1f18297b473"}}},{"cell_type":"code","source":"\"\"\"def compute_all_metrics(df_res_train, df_res_val,dico_metrics,dico_models, temporalite=['d','w','m']):\n  dico_errors = {}\n  for t in temporalite:\n    list_models = list(dico_models.keys())      \n    variables_train = ['pred_' + key for key in list_models] + ['y'] \n    variables_train = [x for x in variables_train if x not in ['y_train_pred_max_TBATS', 'y_train_pred_min_TBATS']]\n    variables = ['pred_' + key for key in list_models] + ['y_min_' + key for key in list_models] + ['y_max_' + key for key in list_models] + ['y']\n    variables_val = [x for x in variables if x not in ['y_min_blending', 'y_max_blending', 'y_min_SARIMAX', 'y_max_SARIMAX']]\n    agg_var_train = {key:'mean' for key in variables_train}\n\n    #variables_val.add('y')\n    agg_var_val = {key:'mean' for key in variables_val}\n\n    df_train = df_res_train.resample(t).agg(agg_var_train)\n    df_val = df_res_val.resample(t).agg(agg_var_val)\n    dico_errors[t] = get_metrics_final(df_train,df_val,dico_metrics,dico_models)\n  return dico_errors\"\"\"","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fb91446-4bcc-49b0-ad35-04176700c01c"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def max_error_normalized(row, model): \n      return (abs(row['pred_' + model] - row['y'])/row['y'])\n    \ndef indicatrice(row, model):\n  if row['y'] <= row['y_max_' + model] and row['y'] >= row['y_min_' + model]:\n    return 1\n  else:\n    return 0","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdb6484d-0033-4bc8-8f62-2cea5ce1ddf1"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_metrics_final(df_res_train, df_res_val,dico_metrics, list_models):\n  y_train = df_res_train.y\n  y_val = df_res_val.y\n  df_errors = pd.DataFrame(columns = list_models)\n  dico_valeurs = {}\n    \n  for model in list_models:\n    \n    y_train_pred = df_res_train.loc[:,'pred_' +model]\n    y_val_pred = df_res_val.loc[:,'pred_' +model]\n\n    dico_errors ={}\n    \n      \n    for metric in dico_metrics:  \n      \n      metric_fun = dico_metrics[metric]\n      \n      if metric == 'max_error':\n        dico_errors[metric+'_train'] = max(df_res_train.apply(lambda row : max_error_normalized(row, model), axis=1))\n        dico_errors[metric+'_val'] = max(df_res_val.apply(lambda row : max_error_normalized(row, model), axis=1))\n        \n      else: \n        dico_errors[metric+'_train'] = metric_fun(y_train,y_train_pred)\n        dico_errors[metric+'_val'] = metric_fun(y_val,y_val_pred)\n    \n    if model not in ['TBATS', 'blending']:\n      dico_errors['IC_ind_val'] = sum(df_res_val.apply(lambda row : indicatrice(row, model), axis=1))*100/len(df_res_val)\n    else : \n      dico_errors['IC_ind_val'] = 0\n      \n    dico_valeurs[model] = dico_errors\n  df_errors = pd.DataFrame(dico_valeurs)\n  return df_errors","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8584a94-3889-410c-9d2a-664dee2741ee"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Selection meilleur modèle","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c590d194-82cb-491a-a330-8c83cdff94a5"}}},{"cell_type":"code","source":"def rank_values(df): \n  dic_met = {}\n  for x in range(len(df)): \n    rank = []\n    list_sort = list(np.sort(list(df.iloc[x,:])))\n    if df.index[x] == 'IC_ind_train' or df.index[x] == 'IC_ind_val':\n      list_sort.reverse()\n    for y in range(len(list_sort)): \n      rank.append(list_sort.index(df.iloc[x,y]))\n    dic_met[df.index[x]] = rank      \n  dict_met = pd.DataFrame(dic_met).T\n  dict_met.columns = df.columns\n  return dict_met","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a1f2ed4-98ab-4601-9699-39aef0210cb2"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_selection(df_metrique_val): \n  df_rank = rank_values(df_metrique_val)\n  classement = []\n  for x in range(df_rank.shape[1]):\n    points = 1/3*df_rank.iloc[list(df_rank.index).index('mae_val'), x] + 1/3*df_rank.iloc[list(df_rank.index).index('max_error_val'), x] + 1/3*df_rank.iloc[list(df_rank.index).index('IC_ind_val'), x]\n    classement.append(points)\n  idx = classement.index(min(classement))\n  modele = list(df_rank.columns)[idx]\n  return(modele)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"304940d6-d183-4a75-8b5f-1aad44f11f9e"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def best_model(df_res_train, df_res_val, df_res_test, model, client): \n  choix = [x for x in df_res_train.columns if model in x] + ['y']\n  train = df_res_train[choix]\n  choix = [x for x in df_res_val.columns if model in x] + ['y']\n  val = df_res_val[choix]\n  choix = [x for x in df_res_test.columns if model in x] \n  test = df_res_test[choix]\n  plot_ts_df(train, val, test,client)\n  return(train, val, test)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ec60f41-6694-489d-bf47-c4675a3b6199"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Transformation df mensuel/annuel","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7490d95e-2c91-48fe-b618-bfc65402e480"}}},{"cell_type":"code","source":"def construct_df_tempo(timeseries, df_, tempo='m'):\n  \"\"\"Prend en entrée la ts initiale et df et renvoie le df mens ou annuel selon temporalité\"\"\"\n  \n  ts = timeseries.copy()\n  df = df_.copy()\n  \n  ts.index = ts.DATE\n  df_join = ts.loc[ts.index.isin(df.index.to_timestamp().to_list())]\n  df.index = df_join.index\n  df_complet = pd.concat([df_join, df], axis=1).reset_index(drop=True)\n\n  for x in [x for x in df_complet.columns if x not in  ['NB_ACTIFS', 'DATE', 'NB_INACTIFS']]:\n    df_complet[x] = df_complet[x]*df_complet['NB_ACTIFS']\n\n  if tempo == \"m\": \n    df_complet['DATE'] = pd.to_datetime(df_complet['DATE']).dt.strftime('%m-%Y')\n  elif tempo == \"y\": \n    df_complet['DATE'] = pd.to_datetime(df_complet['DATE']).dt.strftime('%Y')\n  elif tempo == \"w\": \n    df_complet['DATE'] = pd.to_datetime(df_complet['DATE']).dt.strftime('%Y-%W')\n\n  df_complet = df_complet.drop(columns=['TX_ABS', 'y']).groupby(['DATE']).sum()\n\n  for x in [x for x in df_complet.columns if x not in ['DATE', 'NB_ACTIFS', 'NB_INACTIFS']]: \n    df_complet[x] = df_complet[x] / df_complet['NB_ACTIFS']\n\n  df_complet['y'] = df_complet['NB_INACTIFS'] / df_complet['NB_ACTIFS']\n  df_complet = df_complet.drop(columns=['NB_INACTIFS', 'NB_ACTIFS'])\n  \n  return(df_complet)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0021f2d0-b6bb-4e29-9a42-9e4ed4461c1b"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Assembling","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0c8b0ee-31b0-48fd-93cf-8032de2190ac"}}},{"cell_type":"code","source":"def n_best_model(df, n): \n  classement = []\n  for x in range(df.shape[1]):\n    points = 1/3*df.iloc[list(df.index).index('mae_val'), x] + 1/3*df.iloc[list(df.index).index('max_error_val'), x] + 1/3*df.iloc[list(df.index).index('IC_ind_val'), x]\n    classement.append(points)\n  classement_trie = sorted(classement)\n  best_modeles = []\n  for i in range(n):\n    best_modeles.append(list(df.columns)[classement.index(classement_trie[i])])\n  return(best_modeles)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efc3d85c-3f54-4b7a-8cc1-a9adcd652f4b"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def assembling_models(timeseries, train, val, test, dico_pred, dico_metrics, dico_models):\n\n  ts,_,_ = preprocess_data(timeseries)\n  y = ts[['y']]\n  y_train, y_val = temporal_train_test_split(y, test_size = dico_pred['val_size'])\n  X_idx = pd.period_range(start=train.index.min(),end=test.index.max(), freq=\"d\")\n  train_idx = pd.period_range(start=train.index.min(),end=train.index.max(), freq=\"d\")\n  val_idx = pd.period_range(start=val.index.min(),end=val.index.max(), freq=\"d\")\n  test_idx = pd.period_range(start=test.index.min(),end=test.index.max(), freq=\"d\")\n  \n  X = create_features(X_idx)\n  X_train = X[train.index.min(): train.index.max()]\n  X_val = X[val.index.min():val.index.max()]\n  X_trainval = X[train.index.min(): val.index.max()]\n  X_test = X[test.index.min(): test.index.max()]\n  \n  df_train = train.copy()\n  df_val = val.copy()\n  df_train_mens = construct_df_tempo(timeseries, df_train, 'm')\n  df_val_mens = construct_df_tempo(timeseries, df_val, 'm')\n  metrics_mens = get_metrics_final(df_train_mens,df_val_mens,dico_metrics,list(dico_models.keys()))\n  df = rank_values(metrics_mens)\n  mae=[]\n  \n  for n_models in range(1,len(dico_models)+1):\n    list_models = n_best_model(df, n_models)\n    col = ['pred_' + x for x in list_models] \n    X_train = pd.concat([X_train, train[[x for x in col]]], axis=1)\n    X_val = pd.concat([X_val, val[[x for x in col]]], axis=1)\n  \n\n    #Evaluation sur la validation\n    model = LinearRegression(fit_intercept=True)\n    model.fit(X_train, y_train['y']) #y_train est un df\n    y_val_pred = pd.DataFrame({\"pred_blending\": model.predict(X_val)})\n    y_val_pred.index = y_val.index\n    y_train_pred =  pd.DataFrame({\"pred_blending\": model.predict(X_train)})\n    y_train_pred.index = y_train.index\n    error = mean_absolute_error(y_val['y'], y_val_pred)\n    mae.append(error)\n    if error == min(mae):\n      val_ = y_val_pred\n      train_ = y_train_pred\n      nb_models = n_models\n  \n  list_models = n_best_model(df, nb_models)\n  col = ['pred_' + x for x in list_models] \n  X_trainval = pd.concat([X_trainval, pd.concat([train[[x for x in col]], val[[x for x in col]]], axis=0)], axis=1)\n  X_test = pd.concat([X_test, test[[x for x in col]]], axis=1)\n\n  model = LinearRegression(fit_intercept=True)\n  model.fit(X_trainval, y['y'])\n\n  test_ = pd.DataFrame({\"pred_blending\": model.predict(X_test)})\n  test_.index = test_idx\n   \n  return(train_, val_, test_)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"870d670d-af54-4218-be48-b28203f0bb00"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformation df pour PowerBI","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d70e5e2a-b31a-4535-a45f-a1526b0cccd8"}}},{"cell_type":"code","source":"def transform_final_2(df):\n\n  tf = df[df['reel'].isna()==False].copy()\n  tf['type']='reel'\n  tf['y'] = tf.reel\n  tf = tf.drop(columns=['reel', 'pred','y_min','y_max'])\n\n  tf_na_min = df[df['y_min'].isna()==False].copy()\n  tf_na_min['type']='y_min'\n  tf_na_min['y']=tf_na_min.y_min\n  tf_na_min = tf_na_min.drop(columns=['reel', 'pred','y_min','y_max'])\n\n  tf_na_max = df[df['y_max'].isna()==False].copy()\n  tf_na_max['type']='y_max'\n  tf_na_max['y']=tf_na_max.y_max\n  tf_na_max = tf_na_max.drop(columns=['reel', 'pred','y_min','y_max'])\n\n  tf_na_pred = df[df['pred'].isna()==False].copy()\n  tf_na_pred['type']='pred'\n  tf_na_pred['y']=tf_na_pred.pred\n  tf_na_pred = tf_na_pred.drop(columns=['reel', 'pred','y_min','y_max'])\n\n  tf = pd.concat([tf,tf_na_min,tf_na_max,tf_na_pred])\n  return(tf)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a6c81af-e9a2-4519-ad2a-40039b2a44db"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Fonction boucle finale","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e165152-2c1c-46f6-a901-89fbf7fcdd63"}}},{"cell_type":"code","source":"def df_models(timeseries, dico_pred, dico_models):\n  \n  \"\"\" Prend en entrée la série temporelle, des paramètres de prédiction et une liste de modèle et retourne 1 dicionnaires des meilleurs paramètres pour chaque modèle,\n  et trois dataframes (train, val, test) des prédictions concaténés (avec IC)\"\"\"\n  \n  ts,_,_ = preprocess_data(timeseries)\n  y = ts[['y']]\n  y_train, y_val = temporal_train_test_split(y, test_size = dico_pred['val_size'])\n  #horizons de prédiction\n  fh_train = ForecastingHorizon(y_train.index, is_relative=False)\n  fh_val = ForecastingHorizon(y_val.index, is_relative=False)\n  deb_date_test = y_val.index[-1] + timedelta(days=1)\n  fh_test = ForecastingHorizon(pd.period_range(start=deb_date_test, periods=dico_pred['steps'], freq=\"d\"), is_relative=False)\n  \n  df_train = y_train.copy()\n  df_val = y_val.copy()\n  df_test = pd.DataFrame({'DATE' : pd.period_range(start=deb_date_test, periods=dico_pred['steps'], freq=\"d\")})\n  df_test.index = df_test.DATE\n  \n  dict_bp = {}\n  #boucle sur tous les modèles \n  for model_name in dico_models.keys() : \n\n    param = dico_params[model_name]\n    model = dico_models[model_name]\n         \n    if model_name == 'Prophet': \n      best_params, train, val, test = model_prophet_opti(y, y_train, y_val, dico_pred)\n      \n    elif model_name == 'RL': \n      best_params, train, val, test = model_rl(y, y_train, y_val, dico_pred)\n    \n    elif model_name =='SARIMAX':\n      best_params, train, val, test = model_sarimax(timeseries, val_size, dico_pred['steps'])\n      train = pd.DataFrame(index=y_train.index).merge(train, how='left', right_index=True, left_index=True)\n      train = train.fillna(method='backfill')\n      train = train.fillna(method='ffill')\n      val = pd.DataFrame(index=y_val.index).merge(val, how='left', right_index=True, left_index=True)\n      val = val.fillna(method='backfill')\n      val = val.fillna(method='ffill')\n      test = pd.DataFrame(index=df_test.index).merge(test, how='left', right_index=True, left_index=True)\n      test = test.fillna(method='backfill')\n      test = test.fillna(method='ffill')\n  \n    else:\n      try: \n        cv = ExpandingWindowSplitter(initial_window = 750,step_length = int((len(y)-(840))/3), fh = 90)\n        gscv = ForecastingGridSearchCV(model, cv=cv, param_grid=param, verbose=True, scoring = MeanSquaredError(square_root=True), n_jobs=-1)\n        gscv_train = gscv.fit(y)\n        best_params = gscv_train.best_params_\n      except : \n        cv = ExpandingWindowSplitter(initial_window = 750,step_length = int((len(y)-(840))/3), fh = 90)\n        gscv = ForecastingGridSearchCV(model, cv=cv, param_grid=param, verbose=True, scoring = MeanSquaredError(square_root=True))\n        gscv_train = gscv.fit(y)\n        best_params = gscv_train.best_params_\n      \n      #Fit du modèle sur le train et récupération des fitted_values et de y_val\n      forecaster = model\n      train_model = forecaster.set_params(**best_params).fit(y_train) \n      train = train_model.predict(fh_train).rename(columns={'y' : 'pred'})\n      val = train_model.predict(fh_val).rename(columns={'y' : 'pred'})\n    \n      #Fit sur ts entière et récupération de y_test\n      forecaster = model\n      final_model = forecaster.set_params(**best_params).fit(y)\n      test = final_model.predict(fh_test).rename(columns={'y' : 'pred'})\n\n      #Création des IC, sigma_t = std*(1+alpha*sqrt(t - t_final))\n      #Attention : si les prédiction de base sont pourries sur le val, l'IC peut-être très grand, c'est notamment le cas lorsque le val commence le 1er janvier mais sinon dans tous les autres cas il est fiable\n    \n    coverage = 0.90 #taille de l'IC\n    std = mean_squared_error(y_val['y'], val['pred'], square_root=True)\n    val['time'] = np.arange(len(val))\n    test['time'] = np.arange(len(test))\n    alpha = ((((np.abs(y_val['y'] - val['pred'])/std) - 1)) / (np.sqrt(val['time']))).quantile(coverage) #plus alpha est grand, plus l'IC sera large\n\n    def pred_min(row):\n      return row['pred'] - std*(1+alpha*np.sqrt(row['time']))\n    def pred_max(row):\n      return row['pred'] + std*(1+alpha*np.sqrt(row['time']))\n    \n    test['y_min'] = test.apply(pred_min, axis=1)\n    test['y_max'] = test.apply(pred_max, axis=1)\n    val['y_min'] = val.apply(pred_min, axis=1)\n    val['y_max'] = val.apply(pred_max, axis=1)\n\n    val = val.drop('time', axis=1)\n    test = test.drop('time', axis=1)\n    \n    for x in [train, val, test]: \n      x.columns = [str(col) + '_' + model_name for col in x.columns]\n\n    dict_bp[model_name] = best_params\n    \n    df_train = pd.concat([train, df_train], axis=1)\n    df_val = pd.concat([val, df_val], axis=1)\n    df_test = pd.concat([test, df_test], axis=1)\n    \n  df_test = df_test.drop(columns=['DATE'])\n  \n  return (dict_bp, df_train, df_val, df_test) ","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9476d62-ccdb-4752-bceb-8f996249b5b9"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d03c300-ad57-4db8-979c-d19442e11101"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"      try : \n        cv = ExpandingWindowSplitter(initial_window = 750,step_length = int((len(y_train)-870)/1), fh = 120)\n        gscv = ForecastingGridSearchCV(model, cv=cv, param_grid=param, verbose=True, scoring = MeanSquaredError(square_root=True), n_jobs=-1)\n        gscv_train = gscv.fit(y_train)\n        best_params = gscv_train.best_params_\n        pass\n      except : \n        try : \n          cv = ExpandingWindowSplitter(initial_window = 750,step_length = int((len(y)-(870))/4), fh = 120)\n          gscv = ForecastingGridSearchCV(model, cv=cv, param_grid=param, verbose=True, scoring = MeanSquaredError(square_root=True), n_jobs=-1)\n          gscv_train = gscv.fit(y)\n          best_params = gscv_train.best_params_\n          pass\n        except : \n          try : \n            cv = ExpandingWindowSplitter(initial_window = 750,step_length =50000, fh = 120)\n            gscv = ForecastingGridSearchCV(model, cv=cv, param_grid=param, verbose=True, scoring = MeanSquaredError(square_root=True), n_jobs=-1)\n            gscv_train = gscv.fit(y, fh=fh_train)\n            best_params = gscv_train.best_params_\n            pass\n          except:\n            best_params = {}\n            print('best_params vides')\n            pass\"\"\"","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7d386a2-05e8-43c7-813e-e75a4cd16444"}},"execution_count":null,"outputs":[]}]}